{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e014506-d69d-435a-9552-8fd744e24156",
   "metadata": {},
   "source": [
    "# NLP Information Extraction: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69421783-81a0-409e-8755-43bd6d002814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/commas/anaconda3/envs/ner-vova/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, default_data_collator, get_scheduler\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "random_seed = 42\n",
    "\n",
    "model_name = 'DeepPavlov/rubert-base-cased'  # Baseline\n",
    "model_name = 'ai-forever/ruBert-large'\n",
    "# model_name = 'ai-forever/sbert_large_mt_nlu_ru'\n",
    "# model_name = 'cointegrated/rubert-tiny2'\n",
    "# model_name = 'M-CLIP/M-BERT-Distil-40'\n",
    "# model_name = 'distilbert-base-multilingual-cased'\n",
    "\n",
    "# model_name = 'bert-base-multilingual-uncased'  # Server crashes\n",
    "# model_name = 'DeepPavlov/xlm-roberta-large-en-ru'  # No separation tokens\n",
    "# model_name = 'ai-forever/ruRoberta-large'  # No separation tokens\n",
    "# model_name = 'xlm-roberta-base'  # No separation tokens\n",
    "# model_name = 'ai-forever/rugpt3large_based_on_gpt2'  # NOT for QA\n",
    "\n",
    "model_name_to_save = model_name.split('/')[1] if '/' in model_name else model_name\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "407cbfb5-aad2-4bf7-bdad-10cf1c6829fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_from_disk(\"../data/raw\")\n",
    "test = pd.read_json(f'../data/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d6dca5e-4d4e-49a5-a4b5-a4a6a116addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7ff82e-fe67-4b58-a671-92d5517e5a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "\n",
    "def preprocess_examples(example):\n",
    "    inputs = tokenizer(\n",
    "        example[\"label\"],\n",
    "        example[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    \n",
    "    answer = example[\"extracted_part\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = answer[\"answer_end\"][0]\n",
    "    \n",
    "    context_start = inputs.token_type_ids.index(1)\n",
    "    context_end = len(inputs.token_type_ids) - 2 - inputs.token_type_ids[::-1].index(1)\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    if start_char == end_char:\n",
    "        start_positions.append(start_char)\n",
    "        end_positions.append(start_char)\n",
    "    \n",
    "    else:\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset_mapping[idx][0] <= start_char:\n",
    "            idx += 1   \n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset_mapping[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "    \n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    inputs[\"offset_mapping\"] = offset_mapping\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2f4df1-895e-4f84-9ae4-f6eaf2ecb7ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/vova/nlp-ie/data/raw/train/cache-274b23c804fac426.arrow\n",
      "                                                              \r"
     ]
    }
   ],
   "source": [
    "train_dataset = raw_dataset[\"train\"].map(\n",
    "    preprocess_examples,\n",
    "    remove_columns=raw_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "val_dataset = raw_dataset[\"val\"].map(\n",
    "    preprocess_examples,\n",
    "    remove_columns=raw_dataset[\"val\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f519fcf-26cd-4224-a7d0-687d3d47edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 2\n",
    "\n",
    "num_train_epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8eabe6b-85c5-4f69-a849-7e658485ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset.remove_columns('offset_mapping'),\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset.remove_columns('offset_mapping'), \n",
    "    collate_fn=default_data_collator, \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d2bfd90-fa87-4b78-8471-fb8e592a0b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/commas/anaconda3/envs/ner-vova/lib/python3.9/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ba1206f-e6c8-4476-8b2b-fa6f85729bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dd88e06-0970-4a6f-84cc-72d190d2eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best = 10\n",
    "max_answer_length = 500\n",
    "\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    \n",
    "    predicted_answers = []\n",
    "    for start_logit, end_logit, feature, example in zip(start_logits, end_logits, features, examples):\n",
    "        \n",
    "        example_id = str(example[\"id\"])\n",
    "        context = example[\"text\"]\n",
    "        answers = []\n",
    "        offsets = feature[\"offset_mapping\"]\n",
    "\n",
    "        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                if (\n",
    "                    end_index < start_index\n",
    "                    or end_index - start_index + 1 > max_answer_length\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                answer = {\n",
    "                    \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                    \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                }\n",
    "                answers.append(answer)\n",
    "\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": {'text': ex[\"extracted_part\"][\"text\"], \n",
    "                                                            'answer_start': ex[\"extracted_part\"][\"answer_start\"][0]\n",
    "                                                                 }\n",
    "                           } for ex in examples]\n",
    "    \n",
    "    return np.mean([p['prediction_text']==t['answers']['text'][0] for p,t in zip(predicted_answers, \n",
    "                                                                              theoretical_answers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d5cfa-5a0c-4e03-9d7a-42a20fe35034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "train samples:   0%|          | 0/810 [00:00<?, ?it/s]\u001b[A\n",
      "train samples:   0%|          | 1/810 [00:00<12:11,  1.11it/s]\u001b[A\n",
      "train samples:   0%|          | 3/810 [00:01<03:52,  3.46it/s]\u001b[A\n",
      "train samples:   1%|          | 5/810 [00:01<02:26,  5.51it/s]\u001b[A\n",
      "train samples:   1%|          | 7/810 [00:01<01:51,  7.23it/s]\u001b[A\n",
      "train samples:   1%|          | 9/810 [00:01<01:33,  8.60it/s]\u001b[A\n",
      "train samples:   1%|▏         | 11/810 [00:01<01:22,  9.66it/s]\u001b[A\n",
      "train samples:   2%|▏         | 13/810 [00:01<01:16, 10.46it/s]\u001b[A\n",
      "train samples:   2%|▏         | 15/810 [00:02<01:11, 11.05it/s]\u001b[A\n",
      "train samples:   2%|▏         | 17/810 [00:02<01:09, 11.47it/s]\u001b[A\n",
      "train samples:   2%|▏         | 19/810 [00:02<01:07, 11.78it/s]\u001b[A\n",
      "train samples:   3%|▎         | 21/810 [00:02<01:05, 11.97it/s]\u001b[A\n",
      "train samples:   3%|▎         | 23/810 [00:02<01:04, 12.13it/s]\u001b[A\n",
      "train samples:   3%|▎         | 25/810 [00:02<01:04, 12.24it/s]\u001b[A\n",
      "train samples:   3%|▎         | 27/810 [00:02<01:03, 12.31it/s]\u001b[A\n",
      "train samples:   4%|▎         | 29/810 [00:03<01:03, 12.35it/s]\u001b[A\n",
      "train samples:   4%|▍         | 31/810 [00:03<01:02, 12.39it/s]\u001b[A\n",
      "train samples:   4%|▍         | 33/810 [00:03<01:02, 12.42it/s]\u001b[A\n",
      "train samples:   4%|▍         | 35/810 [00:03<01:02, 12.43it/s]\u001b[A\n",
      "train samples:   5%|▍         | 37/810 [00:03<01:02, 12.46it/s]\u001b[A\n",
      "train samples:   5%|▍         | 39/810 [00:03<01:01, 12.48it/s]\u001b[A\n",
      "train samples:   5%|▌         | 41/810 [00:04<01:01, 12.48it/s]\u001b[A\n",
      "train samples:   5%|▌         | 43/810 [00:04<01:01, 12.49it/s]\u001b[A\n",
      "train samples:   6%|▌         | 45/810 [00:04<01:01, 12.49it/s]\u001b[A\n",
      "train samples:   6%|▌         | 47/810 [00:04<01:01, 12.49it/s]\u001b[A\n",
      "train samples:   6%|▌         | 49/810 [00:04<01:00, 12.49it/s]\u001b[A\n",
      "train samples:   6%|▋         | 51/810 [00:04<01:00, 12.52it/s]\u001b[A\n",
      "train samples:   7%|▋         | 53/810 [00:05<01:00, 12.52it/s]\u001b[A\n",
      "train samples:   7%|▋         | 55/810 [00:05<01:00, 12.51it/s]\u001b[A\n",
      "train samples:   7%|▋         | 57/810 [00:05<01:00, 12.49it/s]\u001b[A\n",
      "train samples:   7%|▋         | 59/810 [00:05<01:00, 12.50it/s]\u001b[A\n",
      "train samples:   8%|▊         | 61/810 [00:05<00:59, 12.49it/s]\u001b[A\n",
      "train samples:   8%|▊         | 63/810 [00:05<00:59, 12.51it/s]\u001b[A\n",
      "train samples:   8%|▊         | 65/810 [00:06<00:59, 12.51it/s]\u001b[A\n",
      "train samples:   8%|▊         | 67/810 [00:06<00:59, 12.51it/s]\u001b[A\n",
      "train samples:   9%|▊         | 69/810 [00:06<00:59, 12.50it/s]\u001b[A\n",
      "train samples:   9%|▉         | 71/810 [00:06<00:59, 12.51it/s]\u001b[A\n",
      "train samples:   9%|▉         | 73/810 [00:06<00:58, 12.51it/s]\u001b[A\n",
      "train samples:   9%|▉         | 75/810 [00:06<00:58, 12.51it/s]\u001b[A\n",
      "train samples:  10%|▉         | 77/810 [00:06<00:58, 12.51it/s]\u001b[A\n",
      "train samples:  10%|▉         | 79/810 [00:07<00:58, 12.50it/s]\u001b[A\n",
      "train samples:  10%|█         | 81/810 [00:07<00:58, 12.48it/s]\u001b[A\n",
      "train samples:  10%|█         | 83/810 [00:07<00:58, 12.49it/s]\u001b[A\n",
      "train samples:  10%|█         | 85/810 [00:07<00:58, 12.49it/s]\u001b[A\n",
      "train samples:  11%|█         | 87/810 [00:07<00:57, 12.49it/s]\u001b[A\n",
      "train samples:  11%|█         | 89/810 [00:07<00:57, 12.49it/s]\u001b[A\n",
      "train samples:  11%|█         | 91/810 [00:08<00:57, 12.52it/s]\u001b[A\n",
      "train samples:  11%|█▏        | 93/810 [00:08<00:57, 12.51it/s]\u001b[A\n",
      "train samples:  12%|█▏        | 95/810 [00:08<00:57, 12.51it/s]\u001b[A\n",
      "train samples:  12%|█▏        | 97/810 [00:08<00:57, 12.51it/s]\u001b[A\n",
      "train samples:  12%|█▏        | 99/810 [00:08<00:56, 12.50it/s]\u001b[A\n",
      "train samples:  12%|█▏        | 101/810 [00:08<00:56, 12.51it/s]\u001b[A\n",
      "train samples:  13%|█▎        | 103/810 [00:09<00:56, 12.51it/s]\u001b[A\n",
      "train samples:  13%|█▎        | 105/810 [00:09<00:56, 12.50it/s]\u001b[A\n",
      "train samples:  13%|█▎        | 107/810 [00:09<00:56, 12.49it/s]\u001b[A\n",
      "train samples:  13%|█▎        | 109/810 [00:09<00:56, 12.50it/s]\u001b[A\n",
      "train samples:  14%|█▎        | 111/810 [00:09<00:55, 12.49it/s]\u001b[A\n",
      "train samples:  14%|█▍        | 113/810 [00:09<00:55, 12.48it/s]\u001b[A\n",
      "train samples:  14%|█▍        | 115/810 [00:10<00:55, 12.48it/s]\u001b[A\n",
      "train samples:  14%|█▍        | 117/810 [00:10<00:55, 12.50it/s]\u001b[A\n",
      "train samples:  15%|█▍        | 119/810 [00:10<00:55, 12.52it/s]\u001b[A\n",
      "train samples:  15%|█▍        | 121/810 [00:10<00:54, 12.54it/s]\u001b[A\n",
      "train samples:  15%|█▌        | 123/810 [00:10<00:54, 12.55it/s]\u001b[A\n",
      "train samples:  15%|█▌        | 125/810 [00:10<00:54, 12.57it/s]\u001b[A\n",
      "train samples:  16%|█▌        | 127/810 [00:10<00:54, 12.52it/s]\u001b[A\n",
      "train samples:  16%|█▌        | 129/810 [00:11<00:54, 12.53it/s]\u001b[A\n",
      "train samples:  16%|█▌        | 131/810 [00:11<00:54, 12.52it/s]\u001b[A\n",
      "train samples:  16%|█▋        | 133/810 [00:11<00:54, 12.53it/s]\u001b[A\n",
      "train samples:  17%|█▋        | 135/810 [00:11<00:53, 12.52it/s]\u001b[A\n",
      "train samples:  17%|█▋        | 137/810 [00:11<00:53, 12.50it/s]\u001b[A\n",
      "train samples:  17%|█▋        | 139/810 [00:11<00:53, 12.50it/s]\u001b[A\n",
      "train samples:  17%|█▋        | 141/810 [00:12<00:53, 12.50it/s]\u001b[A\n",
      "train samples:  18%|█▊        | 143/810 [00:12<00:53, 12.50it/s]\u001b[A\n",
      "train samples:  18%|█▊        | 145/810 [00:12<00:53, 12.51it/s]\u001b[A\n",
      "train samples:  18%|█▊        | 147/810 [00:12<00:53, 12.50it/s]\u001b[A\n",
      "train samples:  18%|█▊        | 149/810 [00:12<00:52, 12.50it/s]\u001b[A\n",
      "train samples:  19%|█▊        | 151/810 [00:12<00:52, 12.51it/s]\u001b[A\n",
      "train samples:  19%|█▉        | 153/810 [00:13<00:52, 12.51it/s]\u001b[A\n",
      "train samples:  19%|█▉        | 155/810 [00:13<00:52, 12.52it/s]\u001b[A\n",
      "train samples:  19%|█▉        | 157/810 [00:13<00:52, 12.51it/s]\u001b[A\n",
      "train samples:  20%|█▉        | 159/810 [00:13<00:52, 12.51it/s]\u001b[A\n",
      "train samples:  20%|█▉        | 161/810 [00:13<00:51, 12.51it/s]\u001b[A\n",
      "train samples:  20%|██        | 163/810 [00:13<00:51, 12.51it/s]\u001b[A\n",
      "train samples:  20%|██        | 165/810 [00:14<00:51, 12.51it/s]\u001b[A\n",
      "train samples:  21%|██        | 167/810 [00:14<00:51, 12.50it/s]\u001b[A\n",
      "train samples:  21%|██        | 169/810 [00:14<00:51, 12.50it/s]\u001b[A\n",
      "train samples:  21%|██        | 171/810 [00:14<00:51, 12.51it/s]\u001b[A\n",
      "train samples:  21%|██▏       | 173/810 [00:14<00:50, 12.50it/s]\u001b[A\n",
      "train samples:  22%|██▏       | 175/810 [00:14<00:50, 12.48it/s]\u001b[A\n",
      "train samples:  22%|██▏       | 177/810 [00:14<00:50, 12.49it/s]\u001b[A\n",
      "train samples:  22%|██▏       | 179/810 [00:15<00:50, 12.49it/s]\u001b[A\n",
      "train samples:  22%|██▏       | 181/810 [00:15<00:50, 12.50it/s]\u001b[A\n",
      "train samples:  23%|██▎       | 183/810 [00:15<00:50, 12.49it/s]\u001b[A\n",
      "train samples:  23%|██▎       | 185/810 [00:15<00:49, 12.52it/s]\u001b[A\n",
      "train samples:  23%|██▎       | 187/810 [00:15<00:49, 12.51it/s]\u001b[A\n",
      "train samples:  23%|██▎       | 189/810 [00:15<00:49, 12.49it/s]\u001b[A\n",
      "train samples:  24%|██▎       | 191/810 [00:16<00:49, 12.51it/s]\u001b[A\n",
      "train samples:  24%|██▍       | 193/810 [00:16<00:49, 12.51it/s]\u001b[A\n",
      "train samples:  24%|██▍       | 195/810 [00:16<00:49, 12.51it/s]\u001b[A\n",
      "train samples:  24%|██▍       | 197/810 [00:16<00:49, 12.50it/s]\u001b[A\n",
      "train samples:  25%|██▍       | 199/810 [00:16<00:48, 12.50it/s]\u001b[A\n",
      "train samples:  25%|██▍       | 201/810 [00:16<00:48, 12.51it/s]\u001b[A\n",
      "train samples:  25%|██▌       | 203/810 [00:17<00:48, 12.50it/s]\u001b[A\n",
      "train samples:  25%|██▌       | 205/810 [00:17<00:48, 12.50it/s]\u001b[A\n",
      "train samples:  26%|██▌       | 207/810 [00:17<00:48, 12.51it/s]\u001b[A\n",
      "train samples:  26%|██▌       | 209/810 [00:17<00:47, 12.55it/s]\u001b[A\n",
      "train samples:  26%|██▌       | 211/810 [00:17<00:47, 12.48it/s]\u001b[A\n",
      "train samples:  26%|██▋       | 213/810 [00:17<00:47, 12.50it/s]\u001b[A\n",
      "train samples:  27%|██▋       | 215/810 [00:18<00:47, 12.50it/s]\u001b[A\n",
      "train samples:  27%|██▋       | 217/810 [00:18<00:47, 12.51it/s]\u001b[A\n",
      "train samples:  27%|██▋       | 219/810 [00:18<00:47, 12.50it/s]\u001b[A\n",
      "train samples:  27%|██▋       | 221/810 [00:18<00:46, 12.54it/s]\u001b[A\n",
      "train samples:  28%|██▊       | 223/810 [00:18<00:46, 12.49it/s]\u001b[A\n",
      "train samples:  28%|██▊       | 225/810 [00:18<00:46, 12.50it/s]\u001b[A\n",
      "train samples:  28%|██▊       | 227/810 [00:18<00:46, 12.50it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(num_train_epochs), desc='epochs'):\n",
    "\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader, desc='train samples'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        start_positions=start_positions,\n",
    "                        end_positions=end_positions)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    for batch in tqdm(val_dataloader, desc='val samples'):\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            start_positions=start_positions,\n",
    "                            end_positions=end_positions)\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    \n",
    "    start_logits = start_logits[: len(val_dataset)]\n",
    "    end_logits = end_logits[: len(val_dataset)]\n",
    "    \n",
    "    metrics = compute_metrics(\n",
    "        start_logits, end_logits, val_dataset, raw_dataset[\"val\"]\n",
    "    )\n",
    "    print(f\"epoch {epoch + 1}:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7efdda-a9d5-4893-8876-f7f636ec95b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_name_to_save + '_8epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a4c9ba-7b1c-4055-8aba-937a75090b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "473b5610-be80-43bd-8085-42c84fd06450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "318it [00:23, 13.65it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "empty_threshold = 30  # number of characters (Postprocessing)\n",
    "\n",
    "for idx, row in tqdm(test.iterrows()):\n",
    "    context = row['text']\n",
    "    label = row['label']\n",
    "    \n",
    "    inputs = tokenizer(label, \n",
    "                       context,\n",
    "                       return_tensors=\"pt\", truncation='only_second',\n",
    "                                   max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "    \n",
    "    start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "    end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]\n",
    "    scores = start_probabilities[:, None] * end_probabilities[None, :]\n",
    "    scores = torch.triu(scores)\n",
    "    max_index = scores.argmax().item()\n",
    "    start_index = max_index // scores.shape[1]\n",
    "    end_index = max_index % scores.shape[1]\n",
    "    \n",
    "    inputs_with_offsets = tokenizer(label, context, return_offsets_mapping=True, truncation='only_second',\n",
    "                                   max_length=max_length)\n",
    "    offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "    \n",
    "    start_char, _ = offsets[start_index]\n",
    "    _, end_char = offsets[end_index]\n",
    "    predicted_answer = context[start_char:end_char]\n",
    "    \n",
    "    # Postprocessing\n",
    "    if len(predicted_answer) <= empty_threshold:\n",
    "        predicted_answer = ''\n",
    "        start_char = 0\n",
    "        end_char = 0\n",
    "\n",
    "    predictions.append({\n",
    "        'text': [predicted_answer],\n",
    "        'answer_start': [start_char],\n",
    "        'answer_end': [end_char]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "99f94601-3910-4965-bf67-87f781f7b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['extracted_part'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "fcdc2bd4-ca73-40e1-8b6d-9ab48c36bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_json(f'predictions_{model_name_to_save}_8epochs_post.json', orient='records', force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2638e5c1-3129-4f79-9f49-2ca20e2cca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python validate.py --predict qa/predictions_$model_name_to_save.json --gt data/test_with_labels.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e53f512-0b39-437a-9b3f-0699deda8121",
   "metadata": {},
   "source": [
    "## Results of QA pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34215e57-b8e6-47ac-a7d6-98a092f5db9d",
   "metadata": {},
   "source": [
    "### cointegrated/rubert-tiny2 (can work with 2048 max_len but were cutted to 512)\n",
    "- Accuracy: 55.35%\n",
    "### M-CLIP/M-BERT-Distil-40\n",
    "- Accuracy: 72.33% (8 epochs)\n",
    "### DeepPavlov/rubert-base-cased (baseline)\n",
    "- Accuracy: 75.80%\n",
    "### distilbert-base-multilingual-cased\n",
    "- Accuracy: 78.62% (8 epochs + postprocessing)\n",
    "### ai-forever/sbert_large_mt_nlu_ru\n",
    "- Accuracy: 83.65%\n",
    "### ai-forever/ruBert-large\n",
    "- Accuracy: 83.02%\n",
    "- Accuracy: 84.91% (8 epochs)\n",
    "\n",
    "**Defaults** = sequence length 512, batch size 2, epochs 4\n",
    "\n",
    "**postprocessing** = make prediction empty if the predicted part length in chars is less than or equal to 30 chars (30 is because I assumed token mean len is 6 while the smallest extracted part is 5 tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a7cb5-77ec-4a12-82c2-a49170f79e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vova2",
   "language": "python",
   "name": "vova2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
